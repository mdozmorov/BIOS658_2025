---
title: "Day 5: Generative Adversarial Networks, Autoencoders, Recurrent Neural Networks, LSTM, GRU, sequence learning"
# linktitle: "1: Introduction to R/RStudio and Images"
date: "2020-06-12"
read_date: "2020-06-12"
draft: true
menu:
  reading:
    parent: Readings
    weight: 5
type: docs
---



<ul>
<li><p><a href="https://www.manning.com/books/deep-learning-with-r">Deep learning with R: Chapter 6.2</a> - Understanding RNN, LSTM, GRU layers</p></li>
<li><p><a href="https://youtu.be/SEnXr6v2ifU">Recurrent Neural Networks | MIT 6.S191</a> - RNN and LSTM overview</p></li>
<li><p>Lipton, Zachary C., John Berkowitz, and Charles Elkan. “<a href="http://arxiv.org/abs/1506.00019">A Critical Review of Recurrent Neural Networks for Sequence Learning</a>.” ArXiv:1506.00019 [Cs], October 17, 2015 - Recurrent Neural network review. Introduction to neural networks, forward and backpropagation, gradient descent. RNN architecture and math, Hopfield networks. LSTMs, Bidirectional RNNs, Neural Turing Machines.</p></li>
<li><p><a href="https://www.manning.com/books/deep-learning-with-r">Deep learning with R: Chapter 8</a> - Generarive deep learning</p></li>
<li><p><a href="https://youtu.be/rZufA635dq4">Deep Generative Modeling | MIT 6.S191</a> - GAN, Autoencoder, Variational autoencoder</p></li>
<li><p><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a> - blog post by Andrej Karpathy</p></li>
<li><p>Hochreiter, Sepp, and Jürgen Schmidhuber. “<a href="https://www.bioinf.jku.at/publications/older/2604.pdf">Long Short-Term Memory</a>.” Neural Computation, 1997 - LSTM paper</p></li>
<li><p><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a></p></li>
<li><p>Hochreiter, Sepp, and Jürgen Schmidhuber. “<a href="https://www.bioinf.jku.at/publications/older/2604.pdf">Long Short-Term Memory</a>.” Neural Computation 9, no. 8 (1997): 1735–1780.</p></li>
<li><p><a href="https://youtu.be/S27pHKBEp30">LSTM is dead. Long Live Transformers!</a> - Crash course into LSTM and transformers, 30min video</p></li>
<li><p><a href="https://towardsdatascience.com/attn-illustrated-attention-5ec4ad276ee3">Attn: Illustrated Attention</a></p></li>
<li><p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. “[Attention Is All You Need](<a href="https://arxiv.org/abs/1706.03762" class="uri">https://arxiv.org/abs/1706.03762</a>” - Transformers paper</p></li>
<li><p><a href="https://openai.com/blog/generative-models/">Generative Models</a> - This post describes four projects that share a common theme of enhancing or using generative models, a branch of unsupervised learning techniques in machine learning</p></li>
<li><p><a href="https://youtu.be/8L11aMN5KY8">A Friendly Introduction to Generative Adversarial Networks (GANs)</a> - 20m video by Luis Serrano. <a href="https://github.com/luisguiserrano/gans">GitHub repo</a></p></li>
<li><p>Goodfellow, Ian J., Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. “<a href="http://arxiv.org/abs/1406.2661">Generative Adversarial Networks</a>.” ArXiv:1406.2661 [Cs, Stat], June 10, 2014</p></li>
<li><p>Goodfellow, Ian. “<a href="http://arxiv.org/abs/1701.00160">NIPS 2016 Tutorial: Generative Adversarial Networks</a>.” ArXiv:1701.00160 [Cs], April 3, 2017</p></li>
<li><p>Pierre Baldi. “<a href="http://proceedings.mlr.press/v27/baldi12a.html">Autoencoders, Unsupervised Learning, and Deep Architectures</a>.” In Proceedings of ICML Workshop on Unsupervised and Transfer Learning, edited by Isabelle Guyon, Gideon Dror, Vincent Lemaire, Graham Taylor, and Daniel Silver, 37–49. PMLR, 2012 - Mathematical framework of autoencoders.</p></li>
<li><p><a href="https://www.analyticsvidhya.com/blog/2020/08/top-5-gan-libraries-you-must-know/">4 Impressive GAN Libraries Every Data Scientist Should Know!</a> by Analytics Vidhya</p></li>
</ul>
